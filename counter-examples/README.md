# Counter Examples

This folder contains experiments demonstrating counter-examples in reinforcement learning â€”  
cases where algorithms behave unexpectedly or fail to converge under specific conditions.

---

## ğŸ“ Structure

```
counter-examples/
â”œâ”€â”€ envs/             # Custom environments designed for counter-examples
â”œâ”€â”€ agents/           # Agents or algorithms that exhibit the issue
â”œâ”€â”€ experiments/      # Scripts reproducing counter-example results
â”œâ”€â”€ utils/            # Helper functions (plotting, logging, analysis)
â”œâ”€â”€ results/          # Plots, logs, and recorded behaviors
â”œâ”€â”€ requirements.txt  # Python dependencies
â””â”€â”€ README.md         # This file
```

---

## âš™ï¸ Setup

1. Install required dependencies:

```bash
pip install -r requirements.txt
```

2. (Optional) Create a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate     # macOS / Linux
.venv\Scripts\activate       # Windows
```

---

## ğŸš€ How to Run

Run an experiment to reproduce a counter-example:

```bash
python experiments/run_counter_example.py --episodes 1000
```

Modify parameters such as:

```bash
python experiments/run_counter_example.py --alpha 0.1 --gamma 0.9 --env special_case
```

Results and visualizations will be saved in the `results/` directory.

---

## ğŸ” Purpose

These examples highlight edge cases where common RL assumptions fail â€”  
for example, divergence in off-policy TD learning or instability in function approximation.

Each experiment aims to show **why** the failure happens and provide intuition behind it.

---

## ğŸ“„ License

This project is open-source under the **MIT License**.  
See the main repository for details.

---

**Author:** [Hakobyan Spartak](https://github.com/HakobyanSpartak)
