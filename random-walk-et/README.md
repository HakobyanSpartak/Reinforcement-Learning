# Random Walk (Eligibility Traces / TD(Î»))

This folder contains code for the Random Walk problem using **eligibility traces** (TD(Î»)) in reinforcement learning.

The goal is to study how the trace decay parameter **Î»** blends nâ€‘step returns and affects learning speed, biasâ€“variance, and stability.

---

## ğŸ“ Structure

```
random-walk-et/
â”œâ”€â”€ envs/             # Random Walk environment
â”œâ”€â”€ agents/           # TD(Î») / eligibility trace agents
â”œâ”€â”€ experiments/      # Training and evaluation scripts
â”œâ”€â”€ utils/            # Plotting, logging, analysis helpers
â”œâ”€â”€ results/          # Saved runs, metrics, figures
â”œâ”€â”€ requirements.txt  # Dependencies
â””â”€â”€ README.md         # This file
```

---

## âš™ï¸ Setup

Install dependencies:

```bash
pip install -r requirements.txt
```

(Optional) create a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate     # macOS / Linux
.venv\Scripts\activate       # Windows
```

---

## ğŸš€ How to Run

Train an agent with TD(Î»):

```bash
python experiments/train_et.py --episodes 1000 --alpha 0.1 --gamma 0.9 --lambda 0.8
```

Evaluate a trained model:

```bash
python experiments/evaluate_et.py --model_path results/best_model.pth
```

Plot learning curves / value error:

```bash
python utils/plot_results.py --input_dir results/ --output_fig et_learning.png
```

Common hyperparameters:
- `--alpha`  learning rate
- `--gamma`  discount factor
- `--lambda` traceâ€‘decay parameter (0 â‰¤ Î» â‰¤ 1)
- `--episodes` number of training episodes

---

## ğŸ” Notes

- TD(Î») unifies oneâ€‘step TD (Î»=0) and Monte Carlo (Î»â†’1) via **eligibility traces**.
- Useful for faster learning and improved credit assignment compared to pure TD(0) or MC in Random Walk.

---

## ğŸ“„ License

Openâ€‘source under the **MIT License** (see repository root).

---

**Author:** [Hakobyan Spartak](https://github.com/HakobyanSpartak)
