{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T22:57:56.247816Z",
     "start_time": "2025-03-02T22:56:46.494761Z"
    }
   },
   "source": [
    "# 10-armed Testbed\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# sys.path.append(r\"C:\\Users\\asus\\Desktop\\ReinforcementLearning\\ten-armed-testbed\")\n",
    "sys.path.append(r\"C:\\Users\\asus\\Desktop\\ReinforcementLearning\\ten-armed-testbed\")\n",
    "\n",
    "# from src.bandit import Bandit\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "# from\n",
    "\n",
    "\n",
    "def simulate(runs, times, bandits):\n",
    "    # region Summary\n",
    "    \"\"\"\n",
    "    For any learning method, we can measure its performance and behavior as it improves with experience over 1000 time steps\n",
    "    when applied to 1 of the bandit problems. This makes up 1 run. Repeating this for 2000 independent runs, each with a different\n",
    "    bandit problem, we obtained measures of the learning algorithm’s average behavior.\n",
    "    :param runs: Number of runs\n",
    "    :param times: Number of times\n",
    "    :param bandits: Bandit problems\n",
    "    :return: Optimal action count mean and reward mean\n",
    "    \"\"\"\n",
    "    # endregion Summary\n",
    "\n",
    "    # region Body\n",
    "\n",
    "    # Prepare a matrix filled with 0s for rewards\n",
    "    rewards = np.zeros((len(bandits), runs, times))\n",
    "\n",
    "    # Prepare a matrix filled with 0s for optimal action counts that has the same shape as rewards matrix\n",
    "    optimal_action_counts = np.zeros(rewards.shape)\n",
    "\n",
    "    # For every bandit\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        # for every run\n",
    "        for run in trange(runs):\n",
    "            # initialize bandit\n",
    "            bandit.initialize()\n",
    "\n",
    "            # for every time step\n",
    "            for time in range(times):\n",
    "                # select an action\n",
    "                action = bandit.act()\n",
    "\n",
    "                # get the reward\n",
    "                rewards[i, run, time] = bandit.step(action)\n",
    "\n",
    "                # if the selected action is optimal for bandit\n",
    "                if action == bandit.optimal_action:\n",
    "                    # change the corresponding 0 in the optimal action counts matrix to 1\n",
    "                    optimal_action_counts[i, run, time] = 1\n",
    "\n",
    "    return optimal_action_counts.mean(axis=1)\n",
    "\n",
    "    # endregion Body\n",
    "\n",
    "\n",
    "## 1. Reward Distribution\n",
    "# Plot an example reward distribution\n",
    "plt.violinplot(dataset=np.random.randn(200, 10) + np.random.randn(10))\n",
    "plt.title(\"Figure 2.1\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Reward distribution\")\n",
    "plt.savefig(\"../generated_images/figure_2_1.png\")\n",
    "plt.close()\n",
    "## 2. Greedy Action Selection VS ε-greedy Action Selection\n",
    "from src.bandit import Bandit\n",
    "\n",
    "# Create a list of epsilons with 0, 0.1 and 0.01 values\n",
    "epsilons = [0, 0.1, 0.01]\n",
    "\n",
    "# Create a list of bandits (1 bandit for every epsilon) where every bandit uses sample-average method\n",
    "bandits = [Bandit(epsilon=e, use_sample_averages=True) for e in epsilons]\n",
    "# Define number of runs\n",
    "runs = 2000\n",
    "\n",
    "# Define number of times\n",
    "time = 1000\n",
    "\n",
    "# Simulate optimal action counts and rewards\n",
    "optimal_action_counts, rewards = simulate(runs, time, bandits)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.subplot(2, 1, 1)\n",
    "for epsilon, rewards in zip(epsilons, rewards):\n",
    "    plt.plot(rewards, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
    "plt.title(\"Figure 2.2\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "for epsilon, counts in zip(epsilons, optimal_action_counts):\n",
    "    plt.plot(counts, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"% Optimal action\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../generated_images/figure_2_2.png\")\n",
    "plt.close()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\e'\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_4900\\4250354648.py:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot(rewards, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_4900\\4250354648.py:100: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot(counts, label=\"$\\epsilon = %.02f$\" % epsilon)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandic class constructor print\n",
      "Bandic class test print\n",
      "Bandic class constructor print\n",
      "Bandic class test print\n",
      "Bandic class constructor print\n",
      "Bandic class test print\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:23<00:00, 83.79it/s]\n",
      "100%|██████████| 2000/2000 [00:23<00:00, 86.59it/s]\n",
      "100%|██████████| 2000/2000 [00:22<00:00, 88.32it/s]\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_4900\\4250354648.py:93: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot(rewards, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_4900\\4250354648.py:100: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot(counts, label=\"$\\epsilon = %.02f$\" % epsilon)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 87\u001B[0m\n\u001B[0;32m     84\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1000\u001B[39m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# Simulate optimal action counts and rewards\u001B[39;00m\n\u001B[1;32m---> 87\u001B[0m optimal_action_counts, rewards \u001B[38;5;241m=\u001B[39m simulate(runs, time, bandits)\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# Plotting\u001B[39;00m\n\u001B[0;32m     90\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m20\u001B[39m))\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
